{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bfcb758-c67e-4cc2-8dbe-3c65e9691e77",
   "metadata": {},
   "source": [
    "# Lab 2 - Gradient-Based Methods for Mean-Variance Portfolio Optimization\n",
    "\n",
    "In this lab, we will explore how to use gradient-based optimization algorithms—Gradient Descent (GD), Stochastic Gradient Descent (SGD), and Adam—to determine optimal portfolio weights in a mean-variance framework. We will compare these gradient based approaches with an analytical least squares optimization algorithm from the Scipy library that has been tradionally used to solve these kinds of problems. \n",
    "\n",
    "The objective of this lab is for you to understand how to compute optimal portfolio weights using optimization algorithms in machine learning when an investor has an option to invest in more than one asset class. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f4d97-6fc7-44d2-8d2f-defc0de0de80",
   "metadata": {},
   "source": [
    "##  Mean-Variance Portfolio Optimization - Problem Formulation\n",
    "\n",
    "1. **Data**: You are provided historical returns for three different asset classes (stocks, bonds and housing). Stock returns are given by S&P composite returns, bond returns by 10 year US Treasury Bills and housing is historical house price index. All data have been retrieved from Robert Shiller's website. All returns are in real terms and span the historical time period 1980-2015 in an annual frequency.\n",
    "   \n",
    "\n",
    "4. **Model Assumptions**:\n",
    "Let $\\mathbf{R}_t \\in \\mathbb{R}^N$ be the return vector at time $t$ for $N$ assets.\n",
    "- Let $\\mu \\in \\mathbb{R}^N$ be the estimated mean returns vector (annual).\n",
    "- Let $\\Sigma \\in \\mathbb{R}^{N \\times N}$ be the estimated covariance matrix of returns.\n",
    "3. **Portfolio Return and Variance**:\n",
    "- Portfolio weights: $\\mathbf{w} \\in \\mathbb{R}^N$, where $\\sum_{i=1}^N w_i=1$ (and often $w_i \\geq 0$ ).\n",
    "- Expected portfolio return:\n",
    "\n",
    "$$\n",
    "\\mu_p=\\mathbf{w}^T \\mu\n",
    "$$\n",
    "\n",
    "- **Portfolio variance**:\n",
    "\n",
    "$$\n",
    "\\sigma_p^2=\\mathbf{w}^T \\Sigma \\mathbf{w}\n",
    "$$\n",
    "\n",
    "4. **Optimization Objective**: The standard portfolio optimization approach is to minimize variance for a given target return or maximize the Sharpe ratio (the excess return over the standard deviation, a return-risk tradeoff measure). For instance, the classic mean-variance formulation can be:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\qquad \\min _{\\mathbf{w}} \\frac{1}{2} \\mathbf{w}^T \\Sigma \\mathbf{w}-\\lambda \\mathbf{w}^T \\mu \\\\\n",
    "& \\text { subject to } \\sum_{i=1}^N w_i=1 \\text { and } w_i \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The first constraint ensures that all the weights sum to 1 (we cannot invest in any other asset class). The second constraint $w_i \\geq 0$ means that no short selling is allowed. \n",
    "\n",
    "- $\\lambda$ is a risk-aversion parameter.\n",
    "- The $\\frac{1}{2}$ is often included for computational convenience (since the derivative of $\\frac{1}{2} \\mathbf{w}^T \\Sigma \\mathbf{w}$ is $\\Sigma \\mathrm{w})$.\n",
    "\n",
    "**Our objective is to compute the portfolio weights $\\mathbf{w} = (w_{Stock}, w_{Bond}, w_{Housing})$.** \n",
    "\n",
    "**We would like to answer this question: How should an investor allocate their wealth between stocks, bonds and housing so that the portfolio risk is minimized for a given portfolio return?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5bae5b-ab7e-4240-86ad-9e3500eb79f4",
   "metadata": {},
   "source": [
    "## Gradient Computation\n",
    "\n",
    "All of the optimization algorithms rely on the us of gradient to minimize the objective function. The gradient is the derivative of\n",
    "the objective function with respect to the weight vector $\\mathbf{w}$.\n",
    "\n",
    "\n",
    "1. Gradient of the objective (ignoring constraints for the moment):\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}}\\left(\\frac{1}{2} \\mathbf{w}^T \\Sigma \\mathbf{w}-\\lambda \\mathbf{w}^T \\mu\\right)=\\Sigma \\mathbf{w}-\\lambda \\mu .\n",
    "$$\n",
    "\n",
    "2. Handling constraints:\n",
    "- Equality constraint $\\sum_i w_i=1$ can be enforced via a Lagrange multiplier or by projecting $\\mathbf{w}$ onto the simplex after each gradient step.\n",
    "- Non-negativity constraint $w_i \\geq 0$ can be handled similarly by projection or by reparameterizing $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557fb5f0-83be-4b34-921e-d681ae5f4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa9275-de46-4001-8042-48049280c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv data \n",
    "\n",
    "\n",
    "# Display the first few rows to check if the formatting is now correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9416b1d-6095-4cbf-bc97-6e0a1c8046fa",
   "metadata": {},
   "source": [
    "- The first column is the year \n",
    "- the second column reports the real returns of S&P composite index\n",
    "- the third column reports the real returns of 10 year Treasury Bonds \n",
    "- and the final column reports the real house price index.\n",
    "\n",
    "We will first create the returns from the house price index by taking the log difference of prices. This creates a missing data in the first year (1890) so we drop this first row and work with the rest of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c7fba-96b5-429d-9eaa-a6b89e76b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute real_house_price returns using simple by log returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a57e6f-a982-4a2a-8b26-d44ca86f6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first row as it contains NaN due to the log difference computation\n",
    "\n",
    "\n",
    "# Drop the original house price index column as it is no longer needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c385187-e143-4de0-b60e-19cc35b5efcc",
   "metadata": {},
   "source": [
    "We are optimizing the mean-variance portfolio using historical returns for stocks, bonds, and house prices. The objective function to minimize is:\n",
    "\n",
    "$$\n",
    "\\min _{\\mathbf{w}} \\frac{1}{2} \\mathbf{w}^T \\Sigma \\mathbf{w}-\\lambda \\mathbf{w}^T \\mu\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "- Sum constraint: $\\sum_{i=1}^N w_i=1$\n",
    "- No short selling: $w_i \\geq 0$ (weights must be non-negative)\n",
    "where:\n",
    "- $\\mathbf{w}=\\left[w_1, w_2, w_3\\right]$ are portfolio weights for stocks, bonds, and house price returns.\n",
    "- $\\mu$ is the vector of mean returns.\n",
    "- $\\Sigma$ is the covariance matrix of returns.\n",
    "- $\\lambda$ is a risk-aversion parameter (higher $\\lambda$ emphasizes return maximization).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5005593-feaa-403c-8000-a374a1a83d41",
   "metadata": {},
   "source": [
    "## Compute Mean and Variance of the Portfolio\n",
    "\n",
    "We will first compute $\\mu$ and $\\Sigma$ from the historical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f16df-e716-412e-a30e-428edff130e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean returns (mu) and covariance matrix (sigma)\n",
    "\n",
    "\n",
    "# Display computed values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9067d4c-2765-43bb-9545-3f0061a69e83",
   "metadata": {},
   "source": [
    "The mean returns for stocks, bonds and housing are 6.75%, 1.7% and 0.35% respectively. The average returns for stocks is much higher than bonds or housing. The actual returns for housing is much higher than those reported here, consider this as a heavily biased version of the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c1b5c0-d121-4ca4-afa3-76c241e8fb08",
   "metadata": {},
   "source": [
    "### Set Optimization Parameters\n",
    "\n",
    "Let's define the number of assets and risk aversion parameter for the problem. This is quite a simple portfolio optimization model in the sense that we have only three assets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff437ccc-ef46-4141-809c-a170a867bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimization problem parameters\n",
    "\n",
    "\n",
    "# Initialize risk aversion parameter as 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0fcee5-04e8-4ae4-ae01-1f08c3512775",
   "metadata": {},
   "source": [
    "### Objective and Gradient Functions \n",
    "\n",
    "We will first create two functions that compute the objective and the gradient of the objective function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf590d-7fa5-4823-b068-8a6a3606a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective_function():\n",
    "    \"\"\"\n",
    "    Compute the mean-variance objective function value.\n",
    "    \n",
    "    Parameters:\n",
    "    w : array-like, shape (N,)\n",
    "        Portfolio weights.\n",
    "    mu : array-like, shape (N,)\n",
    "        Mean returns vector.\n",
    "    Sigma : array-like, shape (N, N)\n",
    "        Covariance matrix.\n",
    "    lambda_risk : float\n",
    "        Risk aversion parameter.\n",
    "    \n",
    "    Returns:\n",
    "    Objective function value.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Define the gradient of the objective function\n",
    "def gradient():\n",
    "    \"\"\"\n",
    "    Compute the gradient of the objective function.\n",
    "    \n",
    "    Parameters:\n",
    "    w : array-like, shape (N,)\n",
    "        Portfolio weights.\n",
    "    mu : array-like, shape (N,)\n",
    "        Mean returns vector.\n",
    "    Sigma : array-like, shape (N, N)\n",
    "        Covariance matrix.\n",
    "    lambda_risk : float\n",
    "        Risk aversion parameter.\n",
    "    \n",
    "    Returns:\n",
    "    Gradient vector.\n",
    "    \"\"\"\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007bb4f-6679-4f87-9b51-8982a2a98710",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We will start with the simple Gradient Descent Algorithm. The algorithm is described below:\n",
    "\n",
    "- **Idea**: Iteratively update the portfolio weights using the full gradient of the objective function computed **from all available data**.\n",
    "- Objective Function:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{w})=\\frac{1}{2} \\mathbf{w}^T \\Sigma_t \\mathbf{w}-\\lambda \\mathbf{w}^T \\mu_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\Sigma_t$ and $\\mu_t$ are the covariance matrix and mean returns vector computed using all historical data up to time $t$.\n",
    "- Gradient Computation:\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{w})=\\Sigma_t \\mathbf{w}-\\lambda \\mu_t\n",
    "$$\n",
    "\n",
    "- **Update Equation**:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(k+1)}=\\mathbf{w}^{(k)}-\\alpha\\left(\\Sigma_t \\mathbf{w}^{(k)}-\\lambda \\mu_t\\right)\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "Key Points:\n",
    "- Uses all available data at each iteration.\n",
    "- Convergence depends on the proper choice of the learning rate $\\alpha$.\n",
    "- Requires projecting $\\mathbf{w}$ to satisfy the constraints (e.g., non-negativity and $\\sum_i w_i=1$ ).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f03cdf-4cbd-4694-9975-b4bb4538be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Implementation\n",
    "def gradient_descent(mu, Sigma, lambda_risk, learning_rate=1e-4, max_iters=5000, tol=1e-8):\n",
    "    N = len(mu)\n",
    "    w = np.ones(N) / N  # start with equal weights\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # updating gradient and weights\n",
    "\n",
    "        \n",
    "        # Projection step: force non-negativity and sum-to-one constraint\n",
    "        \n",
    "        \n",
    "        # stop if gradient norm (L2) is below tolerance \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542fd91-373a-4649-ae31-07f61b02a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent optimization\n",
    "\n",
    "\n",
    "# Display the optimal weights from Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891b41c-6480-4b6f-af85-a4a92e7f016a",
   "metadata": {},
   "source": [
    "The Gradient Descent algorithm imposes equal weights for all three assets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d38ff6c-9988-4469-a065-2f8146524e4d",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "- **Idea**: Updates the portfolio weights using a **random subset (mini-batch) of the assets** (or data points) instead of the full dataset.\n",
    "- Modified Gradient Estimation:\n",
    "\n",
    "$$\n",
    "\\nabla f_{\\operatorname{mini}-\\operatorname{batch}}(\\mathbf{w}) \\approx \\Sigma_{\\operatorname{mini}} \\mathbf{w}_{\\operatorname{mini}}-\\lambda \\mu_{\\operatorname{mini}}\n",
    "$$\n",
    "\n",
    "where the subscript \"mini\" indicates computation on a randomly selected subset of indices.\n",
    "- Update Equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(k+1)}=\\mathbf{w}^{(k)}-\\alpha \\nabla f_{\\text {mini-batch }}\\left(\\mathbf{w}^{(k)}\\right)\n",
    "$$\n",
    "\n",
    "Key Points:\n",
    "- Introduces randomness, which can help avoid local minima.\n",
    "- Often faster per iteration for large datasets but can be noisier.\n",
    "- Also requires projection to enforce constraints.\n",
    "\n",
    "**NOTE: In this problem, we have only 3 assets. So, this is not an example where Gradient Descent will perform worse than the Stochastic Gradient Descent. In larger datasets, with a lot of assets, SGD should perform better**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ebb9a1-86ab-4dfc-8f14-43d00c92c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent (SGD) Implementation\n",
    "def stochastic_gradient_descent(mu, Sigma, lambda_risk, learning_rate=0.000001, max_iters=1000, batch_size=1):\n",
    "    \"\"\"\n",
    "    Perform stochastic gradient descent optimization to find optimal portfolio weights.\n",
    "\n",
    "    Parameters:\n",
    "    mu : array-like, shape (N,)\n",
    "        Mean returns vector.\n",
    "    Sigma : array-like, shape (N, N)\n",
    "        Covariance matrix.\n",
    "    lambda_risk : float\n",
    "        Risk aversion parameter.\n",
    "    learning_rate : float, optional\n",
    "        Step size for gradient updates.\n",
    "    max_iters : int, optional\n",
    "        Maximum number of iterations.\n",
    "    batch_size : int, optional\n",
    "        Number of elements used in each update.\n",
    "\n",
    "    Returns:\n",
    "    array-like : Optimal portfolio weights.\n",
    "    \"\"\"\n",
    "    # Initialize weights equally\n",
    "    w = np.array([1/num_assets] * num_assets)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # Select a random batch of assets\n",
    "        indices = np.random.choice(num_assets, batch_size, replace=False)\n",
    "        grad = np.dot(Sigma[indices][:, indices], w[indices]) - lambda_risk * mu[indices]\n",
    "\n",
    "        # Update weights only for the selected indices\n",
    "        w[indices] -= learning_rate * grad\n",
    "        \n",
    "        # Enforce constraints \n",
    "        w = np.maximum(w, 0)\n",
    "        w /= np.sum(w)\n",
    "\n",
    "    return w\n",
    "\n",
    "# Run SGD optimization\n",
    "optimal_w_sgd = stochastic_gradient_descent(mu, Sigma, lambda_risk_aversion)\n",
    "\n",
    "# Display the optimal weights from Stochastic Gradient Descent\n",
    "optimal_w_sgd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab2755-59c7-4e82-af64-7fbbdf2ddd2f",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent Results are similar to GD results, portfolio allocation is uniform. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3cfd3-9877-4d38-86e1-189eb473c175",
   "metadata": {},
   "source": [
    "### Adam \n",
    "\n",
    "Adam (Adaptive Moment Estimation) is a very popular optimization algorithm in Neural Networks that builds on GD and SGD ideas.  \n",
    "\n",
    "Adam stands for Adaptive Moment Estimation. It combines two main ideas:\n",
    "- Momentum: Instead of using only the current gradient, Adam computes an exponential moving average of past gradients. This helps smooth out noisy updates (like carrying inertia in the direction of descent).\n",
    "- Adaptive Learning Rates: Adam also calculates an exponential moving average of the squared gradients. This lets the algorithm adjust the learning rate for each parameter individually, taking larger steps for parameters with small historical gradients and smaller steps for those with large gradients.\n",
    "\n",
    "- Key Components and Equations:\n",
    "1. Gradient Computation:\n",
    "\n",
    "$$\n",
    "g^{(k)}=\\nabla f\\left(\\mathbf{w}^{(k)}\\right)=\\Sigma \\mathbf{w}^{(k)}-\\lambda \\mu\n",
    "$$\n",
    "\n",
    "2. First Moment Estimate (Momentum):\n",
    "\n",
    "Compute an exponential moving average of the gradients:\n",
    "\n",
    "$$\n",
    "m^{(k)}=\\beta_1 m^{(k-1)}+\\left(1-\\beta_1\\right) g^{(k)}\n",
    "$$\n",
    "\n",
    "- Here, $\\beta_1$ is typically set around 0.9.\n",
    "- In simple words: This averages the past gradients so that the update direction is less erratic.\n",
    "3. Second Moment Estimate (Adaptive Learning Rate):\n",
    "\n",
    "Compute an exponential moving average of the squared gradients:\n",
    "\n",
    "$$\n",
    "v^{(k)}=\\beta_2 v^{(k-1)}+\\left(1-\\beta_2\\right)\\left(g^{(k)}\\right)^2\n",
    "$$\n",
    "\n",
    "- Here, $\\beta_2$ is typically set around 0.999 .\n",
    "- In simple words: This tracks how large the gradients are over time so that if a parameter has a consistently high gradient, its update step will be scaled down.\n",
    "4. Bias Correction:\n",
    "\n",
    "Because $\\boldsymbol{m}^{(k)}$ and $v^{(k)}$ start at zero, we correct the bias:\n",
    "\n",
    "$$\n",
    "\\hat{m}^{(k)}=\\frac{m^{(k)}}{1-\\beta_1^k}, \\quad \\hat{v}^{(k)}=\\frac{v^{(k)}}{1-\\beta_2^k}\n",
    "$$\n",
    "\n",
    "- In simple words: This adjustment compensates for the fact that the moving averages are biased towards zero in the early steps.\n",
    "\n",
    "5. Update Equation:\n",
    "\n",
    "Finally, update the weights:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^{(k+1)}=\\mathbf{w}^{(k)}-\\alpha \\frac{\\hat{m}^{(k)}}{\\sqrt{\\hat{v}^{(k)}}+\\epsilon}\n",
    "$$\n",
    "\n",
    "- $\\alpha$ is the learning rate.\n",
    "- $\\epsilon$ is a small constant (e.g., $1 \\times 10^{-8}$ ) to prevent division by zero.\n",
    "- In simple words: The update adjusts the weights by combining the smoothed direction (momentum) and scaling the step size based on the historical magnitude of the gradients (adaptive learning rates).\n",
    "\n",
    "**Benefits of using Adam**\n",
    "- Smooth Updates: The momentum helps avoid oscillations and noisy updates, making the optimization process smoother.\n",
    "- Adaptive Steps: Each parameter’s update is tailored to its historical behavior, often leading to faster and more reliable convergence.\n",
    "- Robustness: Adam tends to perform well across a variety of problems, making it a popular choice in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87f13347-bf56-4256-8826-58766d1a0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam (Adaptive Moment Estimation)\n",
    "def adam(mu, Sigma, lambda_risk, learning_rate=1e-2, beta1=0.9, beta2=0.999, max_iters=5000, epsilon=1e-8):\n",
    "    N = len(mu)\n",
    "    w = np.ones(N) / N  # initialize equally\n",
    "    m = np.zeros(N)     # first moment vector\n",
    "    v = np.zeros(N)     # second moment vector\n",
    "    for t in range(1, max_iters + 1):\n",
    "        grad = gradient(w, mu, Sigma, lambda_risk)\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        w = w - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        # Projection step: enforce non-negativity and normalization\n",
    "        w = np.maximum(w, 0)\n",
    "        w = w / np.sum(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83dded3-b12f-41c0-abb7-9e49bca576dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get portfolio weights using Adam \n",
    "optimal_w_adam = stochastic_gradient_descent(mu, Sigma, lambda_risk_aversion)\n",
    "\n",
    "optimal_w_adam "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec22c38-0270-4c6d-b4e7-159606830b42",
   "metadata": {},
   "source": [
    "Adam also finds an equally weighted portfolio between the three assets as the optimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e07f2-2984-4af4-89c1-4e6f62aa6f41",
   "metadata": {},
   "source": [
    "### (Approx) Analytical Solution using Scipy Optimize - Sequential Least Squares Programming\n",
    "\n",
    "- Basic Idea:\n",
    "\n",
    "A numerical optimization solver that directly minimizes the objective function while **handling constraints explicitly**.\n",
    "- Objective Function:\n",
    "\n",
    "$$\n",
    "\\min _{\\mathbf{w}} \\frac{1}{2} \\mathbf{w}^T \\Sigma_t \\mathbf{w}-\\lambda \\mathbf{w}^T \\mu_t\n",
    "$$\n",
    "\n",
    "- Constraints:\n",
    "- Budget Constraint:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N w_i=1\n",
    "$$\n",
    "\n",
    "- No Short-Selling:\n",
    "\n",
    "$$\n",
    "w_i \\geq 0 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "Key Points:\n",
    "- Uses Sequential Quadratic Programming to solve the constrained optimization problem.\n",
    "- Finds the optimal solution that strictly satisfies the constraints.\n",
    "- Particularly effective for small- to medium-sized portfolios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5290b-daaf-4d4a-8b5f-eba1603dc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Constraint: sum of weights = 1\n",
    "def constraint(w):\n",
    "    return np.sum(w) - 1\n",
    "\n",
    "# Bounds: No short selling (w >= 0)\n",
    "bounds = [(0, 1)] * num_assets\n",
    "\n",
    "# Initial guess (equal allocation)\n",
    "w_init = np.array([1/num_assets] * num_assets)\n",
    "\n",
    "# Run optimization using Scipy's minimize function\n",
    "result = minimize(\n",
    "    objective_function, \n",
    "    w_init, \n",
    "    args=(mu, Sigma, lambda_risk_aversion), \n",
    "    method='SLSQP', \n",
    "    constraints={'type': 'eq', 'fun': constraint}, \n",
    "    bounds=bounds\n",
    ")\n",
    "\n",
    "# Extract optimal weights\n",
    "optimal_w_scipy = result.x\n",
    "\n",
    "# Display optimal weights from Scipy solver\n",
    "optimal_w_scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b99aea-1c87-4d8e-ab31-0eb6a52867c8",
   "metadata": {},
   "source": [
    "Scipy's constrained optimization algorithm puts all the weight on stocks and very little on either bonds or housing. This result is very different from the other three. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f4b77f-a09f-4d97-8290-25e46a197f54",
   "metadata": {},
   "source": [
    "## Results and Algorithm Comparison\n",
    "\n",
    "- Scipy’s SLSQP Solver is **more robust in identifying the true optimum** by satisfying the constraints exactly and using second-order-like information through its quadratic approximations. In this case, it found that allocating nearly 100% to stocks minimizes the objective function given the historical data.\n",
    "- GD, SGD, and Adam, in contrast, converged to an equal-weighted solution because of factors such as **learning rate, projection steps, and the absence of curvature information**. These methods may have been “stuck” in an interior solution due to their update dynamics, especially in a **low-dimensional space where the extreme optimum is at the boundary**.\n",
    "\n",
    "**Low Dimensional Optimization - 3 Assets**\n",
    "We assumed only 3 assets in this portfolio optimization problem which heavily favours algorithms like Scipy's least squares programming which is almost analytical in nature. Algorithms like GD, SGD and Adam really shine when there are a lot of data points, when we have a lot of assets. \n",
    "- With only three assets, the feasible region (the simplex of weights) is small and any differences in risk and return estimates become very influential.\n",
    "- Corner Solutions: When one asset (stocks in this case) appears significantly more attractive (for example, much higher return or better risk-adjusted performance), the true global optimum may lie at a corner of the feasible region. Scipy’s SLSQP, which is designed to accurately satisfy the Karush-Kuhn-Tucker (KKT) conditions, can identify these corner solutions where almost all weight is allocated to the best asset.\n",
    "\n",
    "**Local vs. Global Optima**\n",
    "- GD, SGD and Adam can get stuck in local minima depending on the initial weights and learning rate.\n",
    "- Scipy's SLSQP aims to find a global minimum with constraint handling.\n",
    "\n",
    "**Differences in Solver Approaches**\n",
    "- Scipy’s SLSQP: This method uses a sequential quadratic programming approach that considers both the objective function and constraints simultaneously. It is very good at navigating the constraints (e.g., non-negativity and sum-to-one) to find the true minimum—even if that minimum lies at the boundary of the feasible region.\n",
    "- Gradient-Based Methods: While they are conceptually simpler, they often require careful tuning and may be more “local” in nature. In a small, low-dimensional space, if the gradient updates are not sufficiently aggressive or if the projection steps are too “blunt,” these methods might converge to a balanced solution instead of exploring the boundaries where the true optimum might lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e24b4a-4bbf-4800-a889-1034f019f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# Asset labels\n",
    "labels = [\"Stocks\", \"Bonds\", \"Housing\"]\n",
    "\n",
    "# Number of asset classes\n",
    "n_assets = len(labels)\n",
    "x = np.arange(n_assets)\n",
    "\n",
    "# Bar width\n",
    "width = 0.2\n",
    "\n",
    "# Create a figure and axis for plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot bars for each optimization method with appropriate offsets\n",
    "ax.bar(x - 1.5*width, optimal_w_gd * 100, width, label=\"GD\")\n",
    "ax.bar(x - 0.5*width, optimal_w_sgd * 100, width, label=\"SGD\")\n",
    "ax.bar(x + 0.5*width, optimal_w_adam * 100, width, label=\"Adam\")\n",
    "ax.bar(x + 1.5*width, optimal_w_scipy * 100, width, label=\"Scipy\")\n",
    "\n",
    "# Set x-axis ticks and labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_ylabel(\"Portfolio Weight (%)\")\n",
    "ax.set_title(\"Portfolio Weights for Different Optimization Algorithms\")\n",
    "ax.legend()\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f91b403-f13e-48d2-bbe7-dfcb709fbd71",
   "metadata": {},
   "source": [
    "## Homework - Portfolio Rebalancing Extension\n",
    "\n",
    "In the above approach, we have assumed that portfolio weights are constant across time, that is no portfolio rebalancing which is unrealistic. Using the historical data provided, extend the portfolio optimization algorithm we have done to incorporate annual portfolio rebalancing starting in the year 1950. Meaning, you optimize for the weights after each period (year here).  Specifically, you are required to:\n",
    "1. Rebalance Portfolio Weights Annually:\n",
    "- For every year starting from 1950, recompute the mean returns vector $\\mu_t$ and covariance matrix $\\Sigma_t$ using all available data from the beginning of the dataset up to that year.\n",
    "- Solve for the optimal portfolio weights $\\mathbf{w}_t$ by minimizing the following mean-variance objective function:\n",
    "\n",
    "$$\n",
    "\\min _{\\mathbf{w}_t} f\\left(\\mathbf{w}_t\\right)=\\frac{1}{2} \\mathbf{w}_t^T \\Sigma_t \\mathbf{w}_t-\\lambda \\mathbf{w}_t^T \\mu_t\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^3 w_{i, t}=1 \\text { and } w_{i, t} \\geq 0 \\text { for } i=1,2,3\n",
    "$$\n",
    "\n",
    "- Here, $\\lambda$ is the risk-aversion parameter, and the three assets correspond to Stocks, Bonds, and Housing.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- Rolling Window Estimation: At each rebalancing year $t$, compute the historical mean and covariance matrix using all data from the start of the dataset until year $t$. \n",
    "- Rebalancing:The first rebalancing should occur in the year 1950. Before 1950, you can assume that the portfolio is held constant.\n",
    "- Constraints Handling:Make sure your optimization method enforces that the weights sum to 1 and are non-negative (i.e., no short selling).\n",
    "- Parameter Tuning: Experiment with learning rates and iteration numbers for GD, SGD, and Adam to ensure convergence. The Scipy solver should automatically handle the constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
